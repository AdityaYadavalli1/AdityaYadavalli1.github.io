<!DOCTYPE html> <html lang="en"> <head> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title>Aditya Yadavalli</title> <meta name="author" content="Aditya Yadavalli"/> <meta name="description" content="A simple, whitespace theme for academics. Based on [*folio](https://github.com/bogoli/-folio) design. "/> <meta name="keywords" content="jekyll, jekyll-theme, academic-website, portfolio-website"/> <link href="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/css/bootstrap.min.css" rel="stylesheet" integrity="sha256-DF7Zhf293AJxJNTmh5zhoYYIMs2oXitRfBjY+9L//AY=" crossorigin="anonymous"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"/> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5.15.4/css/all.min.css" integrity="sha256-mUZM63G8m73Mcidfrv5E+Y61y7a12O5mW4ezU3bxqW4=" crossorigin="anonymous"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/academicons@1.9.1/css/academicons.min.css" integrity="sha256-i1+4qU2G2860dGGIOJscdC30s9beBXjFfzjWLjBRsBg=" crossorigin="anonymous"> <link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/github.css" media="none" id="highlight_theme_light"/> <link rel="shortcut icon" href="data:image/svg+xml,<svg xmlns=%22http://www.w3.org/2000/svg%22 viewBox=%220 0 100 100%22><text y=%22.9em%22 font-size=%2290%22>⚛️</text></svg>"> <link rel="stylesheet" href="/assets/css/main.css"> <link rel="canonical" href="https://adityayadavalli1.github.io/"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/native.css" media="none" id="highlight_theme_dark"/> <script src="/assets/js/theme.js"></script> <script src="/assets/js/dark_mode.js"></script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top"> <div class="container"> <div class="navbar-brand social"> <a href="mailto:%61%64%69%74%79%61.%79%61%64%61%76%61%6C%6C%69@%72%65%73%65%61%72%63%68.%69%69%69%74.%61%63.%69%6E" title="email"><i class="fas fa-envelope"></i></a> <a href="https://www.semanticscholar.org/author/Aditya-Yadavalli/151482275" title="Semantic Scholar" target="_blank" rel="noopener noreferrer"><i class="ai ai-semantic-scholar"></i></a> <a href="https://github.com/AdityaYadavalli1" title="GitHub" target="_blank" rel="noopener noreferrer"><i class="fab fa-github"></i></a> <a href="https://www.linkedin.com/in/aditya-yadavalli-02b7391ab" title="LinkedIn" target="_blank" rel="noopener noreferrer"><i class="fab fa-linkedin"></i></a> <a href="https://twitter.com/AdityaYadavall2" title="Twitter" target="_blank" rel="noopener noreferrer"><i class="fab fa-twitter"></i></a> </div> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item active"> <a class="nav-link" href="/">about<span class="sr-only">(current)</span></a> </li> <li class="nav-item "> <a class="nav-link" href="/publications/">publications</a> </li> <li class="nav-item "> <a class="nav-link" href="/assets/pdf/Aditya_CV.pdf">CV</a> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="fas fa-moon"></i> <i class="fas fa-sun"></i> </button> </li> </ul> </div> </div> </nav> </header> <div class="container mt-5"> <div class="post"> <header class="post-header"> <h1 class="post-title"> <span class="font-weight-bold">Aditya</span> Yadavalli </h1> <p class="desc">Speech/NLP Engineer at <a href="https://karya.in" target="_blank" rel="noopener noreferrer">Karya</a></p> </header> <article> <div class="profile float-right"> <figure> <picture> <img class="img-fluid z-depth-1 rounded" src="/assets/img/IMG_6620.JPG" width="auto" height="auto" alt="IMG_6620.JPG"> </picture> </figure> </div> <div class="clearfix"> <p>I am a Speech/NLP Engineer at Karya. Here, I work on data processing pipelines, quality estimation of crowdsourced datasets, and building assistive tools for crowdsource workers.</p> <p>I also collaborate with <a href="http://alexwarstadt.github.io" target="_blank" rel="noopener noreferrer">Prof. Alex Warstadt</a> on using language models as cognitive models to understand the amount, distribution, and the kind of information conveyed through different streams of human communication, i.e, speech and text.</p> <p>In my free time, I volunteer at <a href="https://www.masakhane.io" target="_blank" rel="noopener noreferrer">Masakhane</a> – a group dedicated to work on improving NLP models for Africa. Along with the rest in the group, I explore how current NLP models do not generalise well, especially when we encounter African languages and accents, and how we can make them better.</p> <p>Previously, I was a visiting researcher at Case Western Reserve University (CWRU), where I collaborated with <a href="https://veratobin.org" target="_blank" rel="noopener noreferrer">Prof. Vera Tobin</a>. There, I worked on evaluating NLP models trained on Child Directed Speech (CDS) and was looking to establish common mistakes that humans and NLP models make when acquiring a new language.</p> <p>Even before that, I was a dual degree student at <a href="https://www.iiit.ac.in" target="_blank" rel="noopener noreferrer">IIIT Hyderabad</a>, where I pursued B.Tech. (Hons) in Computer Science &amp; M.S. by Research in Computational Linguistics. For my M.S. thesis, I explored how closely related languages can be used to improve the performance of low-resource languages at Speech Processing Lab (SPL) with <a href="https://www.iiit.ac.in/people/faculty/anilvuppala/" target="_blank" rel="noopener noreferrer">Prof. Anil Kumar Vuppala</a>.</p> <p>When not at work, you can find me discussing cricket or picking up obscure trivia that no one cares about.</p> </div> <div class="news"> <h2>news</h2> <div class="table-responsive" style="max-height: 20vw"> <table class="table table-sm table-borderless"> <tr> <th scope="row">Oct 17, 2024</th> <td> Paper titled <a href="https://aclanthology.org/2024.emnlp-main.451.pdf" target="_blank" rel="noopener noreferrer">PARIKSHA: Large-Scale Investigation of Human-LLM Evaluator Agreement on Multilingual and Multi-Cultural Data</a> accepted at EMNLP 2024! </td> </tr> <tr> <th scope="row">Mar 28, 2024</th> <td> Paper titled <a href="https://dl.acm.org/doi/10.1145/3663775" target="_blank" rel="noopener noreferrer">Speaking in Terms of Money: Financial Knowledge Acquisition through Speech Data Generation</a> accepted at COMPASS 2024! </td> </tr> <tr> <th scope="row">Mar 25, 2024</th> <td> Paper titled <a href="https://dl.acm.org/doi/abs/10.1145/3630106.3659017" target="_blank" rel="noopener noreferrer">“Akal Badi ya Bias”: An Exploration Study of Gender Bias</a> in Hindi accepted at FAccT 2024! </td> </tr> <tr> <th scope="row">Jan 27, 2024</th> <td> Paper titled <a href="https://aclanthology.org/2024.computel-1.11.pdf" target="_blank" rel="noopener noreferrer">MunTTS : A Text-to-Speech System for Mundari</a> accepted at ComputEL-7 workshop </td> </tr> <tr> <th scope="row">Jan 27, 2024</th> <td> Paper titled <a href="https://aclanthology.org/2024.findings-eacl.142.pdf" target="_blank" rel="noopener noreferrer">AccentFold: A Journey through African Accents for Zero-Shot ASR Adaptation to Target Accents</a> accepted at EACL 2024 Findings </td> </tr> <tr> <th scope="row">Oct 4, 2023</th> <td> Paper titled <a href="https://direct.mit.edu/tacl/article/doi/10.1162/tacl_a_00627/118796/AfriSpeech-200-Pan-African-Accented-Speech-Dataset" target="_blank" rel="noopener noreferrer">AfriSpeech-200: Pan-African Accented Speech Dataset for Clinical and General Domain ASR</a> accepted in TACL &amp; will be presented at EMNLP 2023! </td> </tr> <tr> <th scope="row">May 4, 2023</th> <td> Paper titled <a href="https://arxiv.org/pdf/2305.19589.pdf" target="_blank" rel="noopener noreferrer">SLABERT Talk Pretty One Day: Modeling Second Language Acquisition with BERT</a> accepted at <a href="https://2023.aclweb.org" target="_blank" rel="noopener noreferrer">ACL 2023</a> <img class="emoji" title=":sparkles:" alt=":sparkles:" src="https://github.githubassets.com/images/icons/emoji/unicode/2728.png" height="20" width="20"> </td> </tr> <tr> <th scope="row">May 4, 2023</th> <td> Paper titled <a href="https://aclanthology.org/2023.findings-acl.174/" target="_blank" rel="noopener noreferrer">X-RiSAWOZ: High-Quality End-to-End Multilingual Dialogue Datasets and Few-shot Agents</a> accepted in ACL Findings! </td> </tr> <tr> <th scope="row">Nov 11, 2022</th> <td> Defended my MS thesis! Thanks to my panel – <a href="https://sites.google.com/site/kishoreprahallad/?pli=1" target="_blank" rel="noopener noreferrer">Dr. Kishore Prahallad</a> and <a href="https://www.iiit.ac.in/people/faculty/Chiranjeeviyarra/" target="_blank" rel="noopener noreferrer">Prof. Chiranjeevi Yarra</a> <img class="emoji" title=":tada:" alt=":tada:" src="https://github.githubassets.com/images/icons/emoji/unicode/1f389.png" height="20" width="20"> </td> </tr> <tr> <th scope="row">Oct 3, 2022</th> <td> Paper on <a href="https://ieeexplore.ieee.org/document/10023156" target="_blank" rel="noopener noreferrer">“How Do Phonological Properties Affect Bilingual Automatic Speech Recognition?”</a> accepted at <a href="https://slt2022.org" target="_blank" rel="noopener noreferrer">IEEE SLT 2022</a> <img class="emoji" title=":tada:" alt=":tada:" src="https://github.githubassets.com/images/icons/emoji/unicode/1f389.png" height="20" width="20"> </td> </tr> <tr> <th scope="row">Sep 10, 2022</th> <td> I will visiting Incheon to attend <a href="https://interspeech2022.org" target="_blank" rel="noopener noreferrer">Interspeech</a>! Happy to meet you if you’ll be attending the same. </td> </tr> <tr> <th scope="row">Jul 1, 2022</th> <td> I will be attending <a href="https://2022.naacl.org" target="_blank" rel="noopener noreferrer">NAACL</a> in person. If you are attending too, let’s catch up! </td> </tr> <tr> <th scope="row">Jun 19, 2022</th> <td> Submitted my MS Thesis for review <img class="emoji" title=":smile:" alt=":smile:" src="https://github.githubassets.com/images/icons/emoji/unicode/1f604.png" height="20" width="20"> </td> </tr> <tr> <th scope="row">Jun 15, 2022</th> <td> Paper on “Multi-Task End-to-End Model for Telugu Dialect and Speech Recognition” accepted in Interspeech 2022 <img class="emoji" title=":sparkles:" alt=":sparkles:" src="https://github.githubassets.com/images/icons/emoji/unicode/2728.png" height="20" width="20"> </td> </tr> <tr> <th scope="row">Jun 1, 2022</th> <td> Started working at <a href="https://karya.in" target="_blank" rel="noopener noreferrer">Karya</a> under the mentorship of <a href="https://www.microsoft.com/en-us/research/people/visesha/" target="_blank" rel="noopener noreferrer">Dr. Vivek Seshadri</a> as a Speech/NLP Engineer. </td> </tr> </table> </div> </div> <div class="publications"> <h2>selected publications</h2> <ol class="bibliography"> <li> <div class="row"> <div class="col-sm-2 abbr"><abbr class="badge">EMNLP</abbr></div> <div id="Watts2024PARIKSHAAL" class="col-sm-8"> <div class="title">PARIKSHA: A Large-Scale Investigation of Human-LLM Evaluator Agreement on Multilingual and Multi-Cultural Data</div> <div class="author">Watts, Ishaan, Gumma, Varun,  <em>Yadavalli, Aditya</em>, Seshadri, Vivek, Swaminathan, Manohar, and Sitaram, Sunayana </div> <div class="periodical"> <em>In Proc. EMNLP</em> 2024 </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://aclanthology.org/2024.emnlp-main.451.pdf" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">PDF</a> </div> <div class="abstract hidden"> <p>Evaluation of multilingual Large Language Models (LLMs) is challenging due to a variety of factors – the lack of benchmarks with sufficient linguistic diversity, contamination of popular benchmarks into LLM pre-training data and the lack of local, cultural nuances in translated benchmarks. In this work, we study human and LLM-based evaluation in a multilingual, multi-cultural setting. We evaluate 30 models across 10 Indic languages by conducting 90K human evaluations and 30K LLM-based evaluations and find that models such as GPT-4o and Llama-3 70B consistently perform best for most Indic languages. We build leaderboards for two evaluation settings - pairwise comparison and direct assessment and analyse the agreement between humans and LLMs. We find that humans and LLMs agree fairly well in the pairwise setting but the agreement drops for direct assessment evaluation especially for languages such as Bengali and Odia. We also check for various biases in human and LLM-based evaluation and find evidence of self-bias in the GPT-based evaluator. Our work presents a significant step towards scaling up multilingual evaluation of LLMs.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@article</span><span class="p">{</span><span class="nl">Watts2024PARIKSHAAL</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{PARIKSHA: A Large-Scale Investigation of Human-LLM Evaluator Agreement on Multilingual and Multi-Cultural Data}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Watts, Ishaan and Gumma, Varun and Yadavalli, Aditya and Seshadri, Vivek and Swaminathan, Manohar and Sitaram, Sunayana}</span><span class="p">,</span>
  <span class="na">journal</span> <span class="p">=</span> <span class="s">{In Proc. EMNLP}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2024}</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 abbr"><abbr class="badge">EACL Findings</abbr></div> <div id="Owodunni2024AccentFoldAJ" class="col-sm-8"> <div class="title">AccentFold: A Journey through African Accents for Zero-Shot ASR Adaptation to Target Accents</div> <div class="author">Owodunni, Abraham*,  <em>Yadavalli, Aditya*</em>, Emezue, Chris*, Olatunji, Tobi*, and Mbataku, Clinton </div> <div class="periodical"> <em>In Proc. EACL Findings</em> 2024 </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://aclanthology.org/2024.findings-eacl.142.pdf" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">PDF</a> </div> <div class="abstract hidden"> <p>Despite advancements in speech recognition, accented speech remains challenging. While previous approaches have focused on modeling techniques or creating accented speech datasets, gathering sufficient data for the multitude of accents, particularly in the African context, remains impractical due to their sheer diversity and associated budget constraints. To address these challenges, we propose AccentFold, a method that exploits spatial relationships between learned accent embeddings to improve downstream Automatic Speech Recognition (ASR). Our exploratory analysis of speech embeddings representing 100+ African accents reveals interesting spatial accent relationships highlighting geographic and genealogical similarities, capturing consistent phonological, and morphological regularities, all learned empirically from speech. Furthermore, we discover accent relationships previously uncharacterized by the Ethnologue. Through empirical evaluation, we demonstrate the effectiveness of AccentFold by showing that, for out-of-distribution (OOD) accents, sampling accent subsets for training based on AccentFold information outperforms strong baselines a relative WER improvement of 4.6%. AccentFold presents a promising approach for improving ASR performance on accented speech, particularly in the context of African accents, where data scarcity and budget constraints pose significant challenges. Our findings emphasize the potential of leveraging linguistic relationships to improve zero-shot ASR adaptation to target accents</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@article</span><span class="p">{</span><span class="nl">Owodunni2024AccentFoldAJ</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{AccentFold: A Journey through African Accents for Zero-Shot ASR Adaptation to Target Accents}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Owodunni, Abraham* and Yadavalli, Aditya* and Emezue, Chris* and Olatunji, Tobi* and Mbataku, Clinton}</span><span class="p">,</span>
  <span class="na">journal</span> <span class="p">=</span> <span class="s">{In Proc. EACL Findings}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2024}</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 abbr"><abbr class="badge">ACL</abbr></div> <div id="Yadavalli2023SLABERTTP" class="col-sm-8"> <div class="title">SLABERT Talk Pretty One Day: Modeling Second Language Acquisition with BERT</div> <div class="author"> <em>Yadavalli, Aditya*</em>, Yadavalli, Alekhya*, and Tobin, Vera </div> <div class="periodical"> <em>In Proc. ACL</em> 2023 </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://arxiv.org/pdf/2305.19589.pdf" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">PDF</a> </div> <div class="abstract hidden"> <p>Second language acquisition (SLA) research has extensively studied cross-linguistic transfer, the influence of linguistic structure of a speaker’s native language [L1] on the successful acquisition of a foreign language [L2]. Effects of such transfer can be positive (facilitating acquisition) or negative (impeding acquisition). We find that NLP literature has not given enough attention to the phenomenon of negative transfer. To understand patterns of both positive and negative transfer between L1 and L2, we model sequential second language acquisition in LMs. Further, we build a Mutlilingual Age Ordered CHILDES (MAO-CHILDES) – a dataset consisting of 5 typologically diverse languages, i.e., German, French, Polish, Indonesian, and Japanese – to understand the degree to which native Child-Directed Speech (CDS) [L1] can help or conflict with English language acquisition [L2]. To examine the impact of native CDS, we use the TILT-based cross lingual transfer learning approach established by Papadimitriou and Jurafsky (2020) and find that, as in human SLA, language family distance predicts more negative transfer. Additionally, we find that conversational speech data shows greater facilitation for language acquisition than scripted speech data. Our findings call for further research using our novel Transformer-based SLA models and we would like to encourage it by releasing our code, data, and models</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@article</span><span class="p">{</span><span class="nl">Yadavalli2023SLABERTTP</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{SLABERT Talk Pretty One Day: Modeling Second Language Acquisition with BERT}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Yadavalli, Aditya* and Yadavalli, Alekhya* and Tobin, Vera}</span><span class="p">,</span>
  <span class="na">journal</span> <span class="p">=</span> <span class="s">{In Proc. ACL}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2023}</span><span class="p">,</span>
  <span class="na">volume</span> <span class="p">=</span> <span class="s">{abs/2305.19589}</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 abbr"><abbr class="badge">TACL</abbr></div> <div id="Olatunji2023" class="col-sm-8"> <div class="title">AfriSpeech-200: Pan-African Accented Speech Dataset for Clinical and General Domain ASR</div> <div class="author">Olatunji, Tobi, Afonja, Tejumade,  <em>Yadavalli, Aditya</em>, Emezue, Chris Chinenye, Singh, Sahib, Dossou, Bonaventure F. P., Osuchukwu, Joanne, Osei, Salomey, Tonja, Atnafu Lambebo, Etori, Naome, and Mbataku, Clinton </div> <div class="periodical"> <em>Transactions of the Association for Computational Linguistics</em> 2023 </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://direct.mit.edu/tacl/article-pdf/doi/10.1162/tacl_a_00627/2199588/tacl_a_00627.pdf" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">PDF</a> </div> <div class="abstract hidden"> <p>Africa has a very poor doctor-to-patient ratio. At very busy clinics, doctors could see 30+ patients per day—a heavy patient burden compared with developed countries—but productivity tools such as clinical automatic speech recognition (ASR) are lacking for these overworked clinicians. However, clinical ASR is mature, even ubiquitous, in developed nations, and clinician-reported performance of commercial clinical ASR systems is generally satisfactory. Furthermore, the recent performance of general domain ASR is approaching human accuracy. However, several gaps exist. Several publications have highlighted racial bias with speech-to-text algorithms and performance on minority accents lags significantly. To our knowledge, there is no publicly available research or benchmark on accented African clinical ASR, and speech data is non-existent for the majority of African accents. We release AfriSpeech, 200hrs of Pan-African English speech, 67,577 clips from 2,463 unique speakers across 120 indigenous accents from 13 countries for clinical and general domain ASR, a benchmark test set, with publicly available pre-trained models with SOTA performance on the AfriSpeech benchmark.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@article</span><span class="p">{</span><span class="nl">Olatunji2023</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{AfriSpeech-200: Pan-African Accented Speech Dataset for Clinical and General Domain ASR}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Olatunji, Tobi and Afonja, Tejumade and Yadavalli, Aditya and Emezue, Chris Chinenye and Singh, Sahib and Dossou, Bonaventure F. P. and Osuchukwu, Joanne and Osei, Salomey and Tonja, Atnafu Lambebo and Etori, Naome and Mbataku, Clinton}</span><span class="p">,</span>
  <span class="na">journal</span> <span class="p">=</span> <span class="s">{Transactions of the Association for Computational Linguistics}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2023}</span><span class="p">,</span>
  <span class="na">volume</span> <span class="p">=</span> <span class="s">{11}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{1669-1685}</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 abbr"><abbr class="badge">Interspeech</abbr></div> <div id="Aditya2022Interspeech" class="col-sm-8"> <div class="title">Multi-Task End-to-End Model for Telugu Dialect and Speech Recognition</div> <div class="author"> <em>Yadavalli, Aditya</em>, Mirishkar, Ganesh, and Vuppala, Anil Kumar </div> <div class="periodical"> <em>In Proc. Interspeech</em> 2022 </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://www.isca-speech.org/archive/interspeech_2022/yadavalli22_interspeech.html" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">PDF</a> </div> <div class="abstract hidden"> <p>Conventional Automatic Speech Recognition (ASR) systems are susceptible to dialect variations within a language, thereby adversely affecting the ASR. Therefore, the current practice is to use dialect-specific ASRs. However, dialect-specific information or data is hard to obtain making it difficult to build dialect-specific ASRs. Furthermore, it is cumbersome to maintain multiple dialect-specific ASR systems for each language. We build a unified multi-dialect End-to-End ASR that removes the need for a dialect recognition block and the need to maintain multiple dialect-specific ASRs for three Telugu regional dialects: Telangana, Coastal Andhra, and Rayalaseema. We find that pooling the data and training a multi-dialect ASR benefits the low-resource dialect the most – an improvement of over 9.71% in relative Word Error Rate (WER). Subsequently, we experiment with multi-task ASRs where the primary task is to transcribe the audio and the secondary task is to predict the dialect. We do this by adding a Dialect ID to the output targets. Such a model outperforms naive multi-dialect ASRs by up to 8.24% in relative WER. Additionally, we test this model on a dialect recognition task and find that it outperforms strong baselines by 6.14% in accuracy.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">Aditya2022Interspeech</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Yadavalli, Aditya and Mirishkar, Ganesh and Vuppala, Anil Kumar}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{Proc. Interspeech}</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Multi-Task End-to-End Model for Telugu Dialect and Speech Recognition}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{1387--1391}</span><span class="p">,</span>
  <span class="na">doi</span> <span class="p">=</span> <span class="s">{10.21437/Interspeech.2022-10739}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2022}</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 abbr"><abbr class="badge">NAACL-SRW</abbr></div> <div id="Aditya2022NAACLSRW" class="col-sm-8"> <div class="title">Exploring the Effect of Dialect Mismatched Language Models in Telugu Automatic Speech Recognition</div> <div class="author"> <em>Yadavalli, Aditya</em>, Mirishkar, Ganesh, and Vuppala, Anil Kumar </div> <div class="periodical"> <em>In North American Chapter of the Association of Computational Linguistics Student Research Workshop</em> 2022 </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://aclanthology.org/2022.naacl-srw.36/" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">PDF</a> </div> <div class="abstract hidden"> <p>Previous research has found that Acoustic Models (AM) of an Automatic Speech Recognition (ASR) system are susceptible to dialect variations within a language, thereby adversely affecting the ASR. To counter this, researchers have proposed to build a dialect-specific AM while keeping the Language Model (LM) constant for all the dialects. This study explores the effect of dialect mismatched LM by considering three different Telugu regional dialects: Telangana, Coastal Andhra, and Rayalaseema. We show that dialect variations that surface in the form of a different lexicon, grammar, and occasionally semantics can significantly degrade the performance of the LM under mismatched conditions. Therefore, this degradation has an adverse effect on the ASR even when dialect-specific AM is used. We show a degradation of up to 13.13 perplexity points when LM is used under mismatched conditions. Furthermore, we show a degradation of over 9% and over 15% in Character Error Rate (CER) and Word Error Rate (WER), respectively, in the ASR systems when using mismatched LMs over matched LMs.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">Aditya2022NAACLSRW</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Yadavalli, Aditya and Mirishkar, Ganesh and Vuppala, Anil Kumar}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{North American Chapter of the Association of Computational Linguistics Student Research Workshop}</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Exploring the Effect of Dialect Mismatched Language Models in Telugu
  Automatic Speech Recognition}</span><span class="p">,</span>
  <span class="na">publisher</span> <span class="p">=</span> <span class="s">{Association for Computational Linguistics (ACL)}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2022}</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> </ol> </div> </article> </div> </div> <footer class="fixed-bottom"> <div class="container mt-0"> © Copyright 2025 Aditya Yadavalli. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="noopener noreferrer">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" target="_blank" rel="noopener noreferrer">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="noopener noreferrer">GitHub Pages</a>. Last updated: January 17, 2025. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script async src="https://cdn.jsdelivr.net/npm/@popperjs/core@2.11.2/dist/umd/popper.min.js" integrity="sha256-l/1pMF/+J4TThfgARS6KwWrk/egwuVvhRzfLAMQ6Ds4=" crossorigin="anonymous"></script> <script async src="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/js/bootstrap.min.js" integrity="sha256-SyTu6CwrfOhaznYZPoolVw2rxoY7lKYKQvqbtqN93HI=" crossorigin="anonymous"></script> <script async src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@4/imagesloaded.pkgd.min.js"></script> <script defer src="/assets/js/masonry.js" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.0.6/dist/medium-zoom.min.js" integrity="sha256-EdPgYcPk/IIrw7FYeuJQexva49pVRZNmt3LculEr7zM=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js"></script> <script defer src="/assets/js/common.js"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js"></script> <script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script> </body> </html>